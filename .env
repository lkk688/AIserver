# Main Environment Configuration
# ============================
# Use this file for high-level switches.
# Detailed model configuration is in vllm_config.env

# Load detailed configuration
# Docker Compose will load .env automatically, but we can't easily "include" another file here
# for variable substitution within .env itself.
# However, Docker Compose supports `env_file`. We will update docker-compose.yml to use vllm_config.env.

# This file can be kept simple or used for non-model env vars (like tokens).
# HUGGING_FACE_HUB_TOKEN is usually set in the shell, but can be here too.
HUGGING_FACE_HUB_TOKEN=

# Docker Compose Profiles (Model Selection)
# -----------------------------------------
# Control which services to launch. See vllm_config.env for details.
# Options: all, qwen, vl, cosyvoice, hfserve, litellm
COMPOSE_PROFILES=hfserve,litellm

# HFServe Model Type Selection
# ----------------------------
# Options: gemma3, medgemma, translategemma, embedding, all
# Can be comma-separated list, e.g., "gemma3,embedding"
# Default: gemma3 (if not set)
# WARNING: Running multiple models (e.g. gemma3,embedding) may cause OOM on 32GB GPUs
# Falling back to embedding only for reliable testing
HFSERVE_MODEL_TYPE=medgemma,embedding

# HFServe Embedding Model Selection
# ---------------------------------
# Options:
# - google/embeddinggemma-300m (Default)
# - sentence-transformers/all-MiniLM-L12-v2
# - sentence-transformers/embeddinggemma-300m-medical
HFSERVE_EMBEDDING_MODEL=google/embeddinggemma-300m
