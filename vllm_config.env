# vLLM Configuration File for RTX 5090
# ========================================

# Model Selection / Resource Management
# -------------------------------------
# Control which services launch to save GPU memory.
# Usage: export COMPOSE_PROFILES="profile1,profile2" or set in .env
# Available Profiles:
#   - all        : Launch everything (Requires >32GB VRAM or heavy quantization)
#   - qwen       : Launch vllm-qwen3 (Text Chat)
#   - vl         : Launch vllm-qwen3-vl (Vision)
#   - cosyvoice  : Launch CosyVoice (TTS)
#   - hfserve    : Launch HFServe (Gemma/MedGemma)
#   - litellm    : Launch LiteLLM Gateway
#
# Recommended combinations for 32GB VRAM:
#   COMPOSE_PROFILES=qwen,vl,litellm           (Chat + Vision)
#   COMPOSE_PROFILES=qwen,cosyvoice,litellm    (Chat + TTS)
#   COMPOSE_PROFILES=hfserve,litellm           (Gemma/MedGemma Only)
#
# Default (if not set): Docker Compose will only launch services without profiles (none in this config).
# You MUST set COMPOSE_PROFILES in .env or shell. 
# Example:
# COMPOSE_PROFILES=all

# Model Version Switch
# --------------------
# Uncomment ONE of the following blocks to select the model version.

# --- Qwen 2.5 (Current Stable - Recommended for reliability) ---
# QWEN_MODEL_NAME=Qwen/Qwen2.5-14B-Instruct-AWQ
# QWEN_VL_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct-AWQ
# QUANTIZATION=awq
# GPU_MEMORY_UTILIZATION=0.45
# GPU_MEMORY_UTILIZATION_VL=0.35
# MAX_MODEL_LEN=8192
# VLLM_EXTRA_ARGS=--enable-auto-tool-choice --tool-call-parser hermes

# --- Qwen 3-Next (Experimental) ---
# Reference: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html
# Note: 80B MoE is large. Single 5090 (32GB) requires heavy quantization (e.g. AQLM/AWQ) to fit.
# QWEN_MODEL_NAME=Qwen/Qwen3-Next-80B-A3B-Instruct
# QWEN_VL_MODEL_NAME=Qwen/Qwen3-VL-7B-Instruct-AWQ
# QUANTIZATION=awq
# GPU_MEMORY_UTILIZATION=0.9
# GPU_MEMORY_UTILIZATION_VL=0.05
# MAX_MODEL_LEN=4096
# # MTP (Multi-Token Prediction) and Hermes Tool Parser
# VLLM_EXTRA_ARGS="--speculative-config '{\"method\": \"qwen3_next_mtp\", \"num_speculative_tokens\": 2}' --tool-call-parser hermes --enable-auto-tool-choice"

# --- Qwen 3-Next (INT4 AutoRound) ---
# Reference: https://huggingface.co/Intel/Qwen3-Next-80B-A3B-Instruct-int4-AutoRound
# This version is explicitly quantized for lower memory usage.
# QWEN_MODEL_NAME=Intel/Qwen3-Next-80B-A3B-Instruct-int4-AutoRound
# QWEN_VL_MODEL_NAME=Qwen/Qwen3-VL-7B-Instruct-AWQ
# QUANTIZATION= # AutoRound often doesn't need 'awq' flag if model config handles it, or use 'gptq'/'awq' if specified. Docs say just serve the model.
# GPU_MEMORY_UTILIZATION=0.9
# GPU_MEMORY_UTILIZATION_VL=0.05
# MAX_MODEL_LEN=8192
# VLLM_EXTRA_ARGS="--speculative-config '{\"method\": \"qwen3_next_mtp\", \"num_speculative_tokens\": 2}'"

# --- Unquantized / FP8 (For testing or if AWQ fails) ---
# QWEN_MODEL_NAME=Qwen/Qwen2.5-14B-Instruct
# QWEN_VL_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct
# QUANTIZATION=  # or fp8
# GPU_MEMORY_UTILIZATION=0.4  # May need to lower this for unquantized models
# GPU_MEMORY_UTILIZATION_VL=0.3
# MAX_MODEL_LEN=8192 # Lower context for unquantized to save memory
# VLLM_EXTRA_ARGS=

# Default Active Configuration (Qwen 2.5 AWQ)
QWEN_MODEL_NAME=Qwen/Qwen2.5-14B-Instruct-AWQ
QWEN_VL_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct-AWQ
QUANTIZATION=awq
GPU_MEMORY_UTILIZATION=0.55
GPU_MEMORY_UTILIZATION_VL=0.3
MAX_MODEL_LEN=4096
VLLM_EXTRA_ARGS=--enable-auto-tool-choice --tool-call-parser hermes
